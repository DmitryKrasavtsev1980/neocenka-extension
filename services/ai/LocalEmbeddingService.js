/**
 * –õ–æ–∫–∞–ª—å–Ω—ã–π —Å–µ—Ä–≤–∏—Å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ embedding-–≤–µ–∫—Ç–æ—Ä–æ–≤
 * –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–æ—Å—Ç—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞
 */

class LocalEmbeddingService {
    constructor() {
        this.downloadService = new window.ModelDownloadService();
        this.registry = new window.EmbeddingModelsRegistry();
        this.loadedTokenizers = new Map();
        this.vectorCache = new Map();
        
        console.log('‚úÖ [LocalEmbedding] –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω');
    }

    /**
     * –ì–µ–Ω–µ—Ä–∞—Ü–∏—è embedding-–≤–µ–∫—Ç–æ—Ä–∞ –¥–ª—è —Ç–µ–∫—Å—Ç–∞
     */
    async generateEmbedding(text, modelId = 'paraphrase-multilingual-MiniLM-L12-v2') {
        try {
            // –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
            const cacheKey = `${modelId}:${this.hashString(text)}`;
            if (this.vectorCache.has(cacheKey)) {
                console.log(`üìã [LocalEmbedding] –í–µ–∫—Ç–æ—Ä—ã –≤–∑—è—Ç—ã –∏–∑ –∫—ç—à–∞ –¥–ª—è —Ç–µ–∫—Å—Ç–∞ (${text.length} —Å–∏–º–≤–æ–ª–æ–≤)`);
                return this.vectorCache.get(cacheKey);
            }


            // –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –ª–∏ –º–æ–¥–µ–ª—å
            const isDownloaded = await this.downloadService.isModelDownloaded(modelId);
            if (!isDownloaded) {
                throw new Error(`–ú–æ–¥–µ–ª—å ${modelId} –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å —Å–Ω–∞—á–∞–ª–∞.`);
            }

            // –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –º–æ–¥–µ–ª–∏
            const modelConfig = this.registry.getModel(modelId);
            if (!modelConfig) {
                throw new Error(`–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ ${modelId} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞`);
            }

            // –ü–æ–ª—É—á–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            const tokenizer = await this.getTokenizer(modelId);

            // –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç
            const tokens = await this.tokenizeText(text, tokenizer);

            // –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º embedding-–≤–µ–∫—Ç–æ—Ä
            const vector = await this.computeEmbedding(tokens, modelConfig);

            // –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            this.vectorCache.set(cacheKey, vector);

            // –û–±–Ω–æ–≤–ª—è–µ–º –≤—Ä–µ–º—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏
            await this.downloadService.updateLastUsed(modelId);

            return vector;

        } catch (error) {
            console.error(`‚ùå [LocalEmbedding] –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ embedding:`, error);
            throw error;
        }
    }

    /**
     * –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –¥–ª—è –º–æ–¥–µ–ª–∏
     */
    async getTokenizer(modelId) {
        if (this.loadedTokenizers.has(modelId)) {
            return this.loadedTokenizers.get(modelId);
        }

        try {
            // –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
            const tokenizerConfigFile = await this.downloadService.getModelFile(modelId, 'tokenizer_config.json');
            const tokenizerConfig = JSON.parse(new TextDecoder().decode(tokenizerConfigFile.data));

            // –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            const tokenizer = new SimpleTokenizer(tokenizerConfig);
            
            // –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å vocab
            try {
                const vocabFile = await this.downloadService.getModelFile(modelId, 'vocab.txt');
                const vocabText = new TextDecoder().decode(vocabFile.data);
                tokenizer.loadVocab(vocabText);
            } catch (error) {
                // –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –≤–º–µ—Å—Ç–æ vocab
            }

            this.loadedTokenizers.set(modelId, tokenizer);
            return tokenizer;

        } catch (error) {
            console.error(`‚ùå [LocalEmbedding] –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –¥–ª—è ${modelId}:`, error);
            
            // Fallback –∫ –ø—Ä–æ—Å—Ç–æ–º—É —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—É
            const fallbackTokenizer = new SimpleTokenizer({});
            this.loadedTokenizers.set(modelId, fallbackTokenizer);
            return fallbackTokenizer;
        }
    }

    /**
     * –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
     */
    async tokenizeText(text, tokenizer) {
        return tokenizer.tokenize(text);
    }

    /**
     * –í—ã—á–∏—Å–ª–µ–Ω–∏–µ embedding-–≤–µ–∫—Ç–æ—Ä–∞ –∏–∑ —Ç–æ–∫–µ–Ω–æ–≤
     */
    async computeEmbedding(tokens, modelConfig) {
        const dimensions = modelConfig.dimensions || 384;
        
        // –°–æ–∑–¥–∞–µ–º –≤–µ–∫—Ç–æ—Ä –∑–∞–¥–∞–Ω–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
        const vector = new Array(dimensions).fill(0);
        
        // –ü—Ä–æ—Å—Ç–æ–π –∞–ª–≥–æ—Ä–∏—Ç–º –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ embedding –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–æ–∫–µ–Ω–æ–≤
        for (let i = 0; i < tokens.length; i++) {
            const token = tokens[i];
            const tokenId = this.getTokenId(token);
            
            // –†–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∞ –ø–æ –≤–µ–∫—Ç–æ—Ä—É
            for (let dim = 0; dim < dimensions; dim++) {
                const influence = Math.sin((tokenId + i) * (dim + 1) * 0.01);
                vector[dim] += influence / Math.sqrt(tokens.length);
            }
        }

        // –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–µ–∫—Ç–æ—Ä
        const norm = Math.sqrt(vector.reduce((sum, val) => sum + val * val, 0));
        if (norm > 0) {
            for (let i = 0; i < dimensions; i++) {
                vector[i] /= norm;
            }
        }

        return vector;
    }

    /**
     * –ü—Ä–æ—Å—Ç–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞ –≤ ID
     */
    getTokenId(token) {
        let hash = 0;
        for (let i = 0; i < token.length; i++) {
            const char = token.charCodeAt(i);
            hash = ((hash << 5) - hash) + char;
            hash = hash & hash; // –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ 32-–±–∏—Ç–Ω–æ–µ —á–∏—Å–ª–æ
        }
        return Math.abs(hash);
    }

    /**
     * –ü–∞–∫–µ—Ç–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è embedding-–≤–µ–∫—Ç–æ—Ä–æ–≤
     */
    async batchGenerateEmbeddings(texts, modelId, progressCallback = null) {
        const results = [];
        const total = texts.length;


        for (let i = 0; i < texts.length; i++) {
            try {
                const vector = await this.generateEmbedding(texts[i], modelId);
                results.push(vector);

                if (progressCallback && i % 10 === 0) {
                    progressCallback({
                        stage: 'processing',
                        progress: Math.round((i / total) * 100),
                        processed: i + 1,
                        total
                    });
                }
            } catch (error) {
                console.error(`‚ùå [LocalEmbedding] –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ ${i}:`, error);
                // –í–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω—É–ª–µ–≤–æ–π –≤–µ–∫—Ç–æ—Ä –ø—Ä–∏ –æ—à–∏–±–∫–µ
                const modelConfig = this.registry.getModel(modelId);
                const dimensions = modelConfig?.dimensions || 384;
                results.push(new Array(dimensions).fill(0));
            }
        }

        if (progressCallback) {
            progressCallback({
                stage: 'completed',
                progress: 100,
                processed: total,
                total
            });
        }

        return results;
    }

    /**
     * –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞
     */
    cosineSimilarity(vector1, vector2) {
        if (vector1.length !== vector2.length) {
            throw new Error('–í–µ–∫—Ç–æ—Ä—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –æ–¥–∏–Ω–∞–∫–æ–≤–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏');
        }

        let dotProduct = 0;
        let norm1 = 0;
        let norm2 = 0;

        for (let i = 0; i < vector1.length; i++) {
            dotProduct += vector1[i] * vector2[i];
            norm1 += vector1[i] * vector1[i];
            norm2 += vector2[i] * vector2[i];
        }

        if (norm1 === 0 || norm2 === 0) return 0;
        
        return dotProduct / (Math.sqrt(norm1) * Math.sqrt(norm2));
    }

    /**
     * –•—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏ –¥–ª—è –∫—ç—à–∞
     */
    hashString(str) {
        let hash = 0;
        for (let i = 0; i < str.length; i++) {
            const char = str.charCodeAt(i);
            hash = ((hash << 5) - hash) + char;
            hash = hash & hash;
        }
        return hash;
    }

    /**
     * –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞
     */
    clearCache() {
        const cacheSize = this.vectorCache.size;
        this.vectorCache.clear();
        console.log(`üßπ [LocalEmbedding] –û—á–∏—â–µ–Ω –∫—ç—à –≤–µ–∫—Ç–æ—Ä–æ–≤ (${cacheSize} –∑–∞–ø–∏—Å–µ–π)`);
        return cacheSize;
    }

    /**
     * –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫—ç—à–∞
     */
    getCacheStats() {
        return {
            totalEntries: this.vectorCache.size,
            loadedTokenizers: this.loadedTokenizers.size
        };
    }
}

/**
 * –ü—Ä–æ—Å—Ç–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
 */
class SimpleTokenizer {
    constructor(config = {}) {
        this.config = config;
        this.vocab = null;
        this.maxLength = config.max_length || 512;
    }

    loadVocab(vocabText) {
        this.vocab = new Map();
        const lines = vocabText.split('\n');
        
        for (let i = 0; i < lines.length; i++) {
            const token = lines[i].trim();
            if (token) {
                this.vocab.set(token, i);
            }
        }
        
        console.log(`‚úÖ [SimpleTokenizer] –ó–∞–≥—Ä—É–∂–µ–Ω —Å–ª–æ–≤–∞—Ä—å: ${this.vocab.size} —Ç–æ–∫–µ–Ω–æ–≤`);
    }

    tokenize(text) {
        if (!text) return [];

        // –ü—Ä–æ—Å—Ç–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –ø–æ –ø—Ä–æ–±–µ–ª–∞–º –∏ –∑–Ω–∞–∫–∞–º –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        const tokens = text
            .toLowerCase()
            .replace(/[^\w–∞-—è—ë\s]/g, ' ')
            .split(/\s+/)
            .filter(token => token.length > 0);

        // –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
        return tokens.slice(0, this.maxLength);
    }

    encode(text) {
        const tokens = this.tokenize(text);
        
        if (!this.vocab) {
            // –ü—Ä–æ—Å—Ç–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –±–µ–∑ —Å–ª–æ–≤–∞—Ä—è
            return tokens.map(token => this.hashToken(token));
        }

        // –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–ª–æ–≤–∞—Ä—è
        return tokens.map(token => this.vocab.get(token) || this.vocab.get('[UNK]') || 0);
    }

    hashToken(token) {
        let hash = 0;
        for (let i = 0; i < token.length; i++) {
            const char = token.charCodeAt(i);
            hash = ((hash << 5) - hash) + char;
            hash = hash & hash;
        }
        return Math.abs(hash) % 30000; // –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è
    }
}

// –≠–∫—Å–ø–æ—Ä—Ç –≤ –≥–ª–æ–±–∞–ª—å–Ω—É—é –æ–±–ª–∞—Å—Ç—å –≤–∏–¥–∏–º–æ—Å—Ç–∏
window.LocalEmbeddingService = LocalEmbeddingService;